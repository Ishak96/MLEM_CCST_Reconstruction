{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dbba969",
   "metadata": {},
   "source": [
    "# **Setup the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb1640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from math import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Callback, Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "from kornia.losses import ssim_loss, psnr_loss\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bdfb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f9910fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Available GPUs:  1\n",
      "Num Available workerss :  16\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = max(0, torch.cuda.device_count())\n",
    "BATCH_SIZE = 1\n",
    "NUM_WORKERS = int(os.cpu_count())\n",
    "\n",
    "# Image size that we are going to use\n",
    "N = 128\n",
    "\n",
    "N_detectors = 4\n",
    "N_THETA = 179\n",
    "N_RHO = int(ceil(pi*2*N)/N_detectors);\n",
    "NB_PROJECTIONS = N_THETA * N_RHO\n",
    "\n",
    "# Our images are graysacle (1 channels)\n",
    "N_CHANNELS = 1\n",
    "\n",
    "N_ITER = 10\n",
    "\n",
    "SAVE_TEST = \"results/test/L_MLEM%i\" % N + \"_N_ITER_%i\" % N_ITER +\"/\"\n",
    "    \n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"models/L_MLEM/\")\n",
    "SAVE_NAME = \"L_MLEM_%i\" % N + \"_N_ITER_%i\" % N_ITER +\"/\"\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_PATH, SAVE_NAME)\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.makedirs(CHECKPOINT_PATH)\n",
    "    \n",
    "print(\"Num Available GPUs: \", AVAIL_GPUS)\n",
    "print(\"Num Available workerss : \", NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0bcd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set file :  data/Lung-CCST-128-4detectors-data.npy\n",
      "System matrix file :  data/CCST-System-matrix.txt\n",
      "Check point file :  models/L_MLEM/L_MLEM_128_N_ITER_10/\n",
      "Results file :  results/test/L_MLEM128_N_ITER_10/\n"
     ]
    }
   ],
   "source": [
    "PATH_DATASETS = 'Lung-CCST-128-4detectors-data.npy'\n",
    "PATH_SYSTEM_MATRIX = 'CCST-System-matrix.txt'\n",
    "\n",
    "if USE_COLAB: \n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    PATH_DATASETS = \"gdrive/MyDrive/\" + PATH_DATASETS\n",
    "    PATH_SYSTEM_MATRIX = \"gdrive/MyDrive/\" + PATH_SYSTEM_MATRIX\n",
    "else:\n",
    "    PATH_DATASETS = \"data/\" + PATH_DATASETS\n",
    "    PATH_SYSTEM_MATRIX = \"data/\" + PATH_SYSTEM_MATRIX\n",
    "\n",
    "print(\"Data set file : \", PATH_DATASETS)\n",
    "print(\"System matrix file : \", PATH_SYSTEM_MATRIX)\n",
    "print(\"Check point file : \", CHECKPOINT_PATH)\n",
    "print(\"Results file : \", SAVE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0279207",
   "metadata": {},
   "source": [
    "# **Utility function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7815d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_func(display_list, save=False, epoch=0):\n",
    "    plt.figure(figsize=(25, 25))\n",
    "\n",
    "    predicted=\"Predicted Object : \"\n",
    "\n",
    "    if len(display_list) > 2 :\n",
    "        psnr_p = -psnr_loss(display_list[1].cpu(), display_list[2].cpu(), 1)\n",
    "        ssim_p = -2 * ssim_loss(display_list[1].cpu(), display_list[2].cpu(), 5) + 1\n",
    "        predicted = predicted + \", PSNR=\" + str('%.2f' % psnr_p) + \", SSIM=\" + str('%.2f' % ssim_p)\n",
    "\n",
    "    title = [\"Input Data\", 'True Mask', predicted]\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(torch.squeeze(display_list[i]).cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c20d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_system_torch(y, sys_mat, n, nr, nt):\n",
    "    y = torch.reshape(y, (n*n, 1))\n",
    "    d = torch.mm(sys_mat, y)\n",
    "    d = torch.reshape(d, (nr, nt))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48b83b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_system_torch(d, sys_mat, n, nr, nt):\n",
    "    d = torch.reshape(d, (nr*nt, 1))\n",
    "    y = torch.mm(sys_mat.T, d)\n",
    "    y = torch.reshape(y, (n, n))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f9383a",
   "metadata": {},
   "source": [
    "# **Dataset Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da100baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the numpy data array\n",
    "with open(PATH_DATASETS, 'rb') as f:\n",
    "    X_dataset = np.load(f)\n",
    "    Y_dataset = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = len(X_dataset)\n",
    "\n",
    "train_size = int(0.95 * DATASET_SIZE)\n",
    "val_size = DATASET_SIZE - train_size\n",
    "\n",
    "tensor_x = torch.Tensor(X_dataset)\n",
    "tensor_y = torch.Tensor(Y_dataset)\n",
    "\n",
    "dataset = TensorDataset(tensor_x, tensor_y) # create your datset\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_set = DataLoader(train_set)\n",
    "val_set = DataLoader(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1768637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size all dataset: \", len(dataset))\n",
    "print(\"Size training dataset: \", len(train_set))\n",
    "print(\"Size testing dataset: \", len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e215f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inp, y_re = next(iter(train_set))\n",
    "display_func([x_inp, y_re])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52486617",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCSTDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = PATH_DATASETS, batch_size: int = BATCH_SIZE, num_workers: int = NUM_WORKERS):\n",
    "        super(CCSTDataModule, self).__init__()\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "      # download\n",
    "      # load the numpy data array\n",
    "        print(\"BEGIN__Loading the dataset__\")\n",
    "        with open(self.data_dir, 'rb') as f:\n",
    "            X_dataset = torch.Tensor(np.load(f))\n",
    "            Y_dataset = torch.Tensor(np.load(f))\n",
    "        print(\"__DONE__Loading the dataset__\")\n",
    "\n",
    "        self.dataset = TensorDataset(X_dataset, Y_dataset) # create the datset\n",
    "      \n",
    "        self.dataset_size = len(self.dataset)\n",
    "        self.train_size = int(0.90 * self.dataset_size)\n",
    "        self.val_size = int(0.5 * (self.dataset_size - self.train_size))\n",
    "        self.test_size = self.dataset_size - (self.train_size + self.val_size)\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "      # Assign train/val datasets\n",
    "      if stage == \"fit\" or stage is None:\n",
    "            self.train_set, self.val_set, _ = random_split(self.dataset, [self.train_size, self.val_size, self.test_size])\n",
    "\n",
    "      # Assign test dataset\n",
    "      if stage == \"test\" or stage is None:\n",
    "            _, _, self.test_set = random_split(self.dataset, [self.train_size, self.val_size, self.test_size])\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b349f7",
   "metadata": {},
   "source": [
    "# **Build the MLEM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbfa02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.CNN_cell = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 7, padding=(3,3)), nn.PReLU(),\n",
    "            nn.Conv2d(8, 8, 7, padding=(3,3)), nn.PReLU(),\n",
    "            nn.Conv2d(8, 8, 7, padding=(3,3)), nn.PReLU(),\n",
    "            nn.Conv2d(8, 8, 7, padding=(3,3)), nn.PReLU(),\n",
    "            nn.Conv2d(8, 1, 7, padding=(3,3)), nn.PReLU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_t):\n",
    "        x = torch.squeeze(self.CNN_cell(x_t.unsqueeze(0).unsqueeze(0)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c9533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learned_MLEM(nn.Module):\n",
    "    def __init__(self, n, nr, nt, num_its, sys_mat_path=PATH_SYSTEM_MATRIX):\n",
    "        super(Learned_MLEM, self).__init__()\n",
    "        self.n = n\n",
    "        self.nr = nr\n",
    "        self.nt = nt\n",
    "        \n",
    "        self.sys_mat = torch.Tensor(np.loadtxt(open(PATH_SYSTEM_MATRIX, \"rb\"), delimiter=\",\")).T\n",
    "        self.num_its = num_its\n",
    "        self.cnn = CNN()\n",
    "        \n",
    "    def forward(self, d):\n",
    "        d = d.T\n",
    "        self.sys_mat = sys_mat.type_as(d)\n",
    "        \n",
    "        y = torch.ones(self.n, self.n).type_as(d)\n",
    "        data_ones = torch.ones_like(d)\n",
    "        sens_y = bp_system_torch(data_ones, self.sys_mat, self.n, self.nr, self.nt)\n",
    "        \n",
    "        for it in range(self.num_its):\n",
    "            fpdata = fp_system_torch(y, self.sys_mat, self.n, self.nr, self.nt)\n",
    "            ration = d / (fpdata + 1.0e-9)\n",
    "            correction = bp_system_torch(ratio, self.sys_mat, self.n, self.nr, self.nt)\n",
    "            y = y * correction\n",
    "            y = torch.abs(y + self.cnn(y))\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800fd196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LP_Learned_MLEM(LightningModule):\n",
    "    def __init__(self, n, nr, nt, num_its, \n",
    "                lr: float = 0.0002,\n",
    "                b1: float = 0.5,\n",
    "                b2: float = 0.999,\n",
    "                 **kwargs):\n",
    "        super(LP_Learned_MLEM, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.l_mlem = Learned_MLEM(n, nr, nt, num_its)\n",
    "    \n",
    "    def forward(self, d):\n",
    "        return self.l_mlem(d)\n",
    "    \n",
    "    def L_MLEM_loss(y, y_true):\n",
    "        mse_loss = nn.MSELoss()\n",
    "        return mse_loss(y, y_true)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        opt_lmlem = self.optimizers()\n",
    "        \n",
    "        d, y_true = batch\n",
    "\n",
    "        y = self(d)\n",
    "\n",
    "        ######################\n",
    "        # Optimize L_MLEM    #\n",
    "        ######################\n",
    "        # compute losses\n",
    "        lmlem_loss = self.L_MLEM_loss(y, y_true)\n",
    "            \n",
    "        opt_lmlem.zero_grad()\n",
    "        self.manual_backward(lmlem_loss)\n",
    "        opt_lmlem.step()\n",
    "        \n",
    "        self.log_dict({\"Learned_MLEM_loss\": lmlem_loss}, prog_bar=True)         \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        d, y_true = batch\n",
    "\n",
    "        y = self(d)\n",
    "        val_lmlem_loss = self.L_MLEM_loss(y, y_true)\n",
    "\n",
    "        self.log_dict({\"val_Learned_MLEM_loss\": val_lmlem_loss}, prog_bar=True)        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # check if the saving dir exist else create it\n",
    "        if not os.path.exists(SAVE_TEST):\n",
    "            os.makedirs(SAVE_TEST)\n",
    "        \n",
    "        d, y_true = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            y = self(d)\n",
    "            self.train()\n",
    "            test_lmlem_loss = self.L_MLEM_loss(y, y_true)\n",
    "            \n",
    "            self.log_dict({\"test_Learned_MLEM_loss\": test_lmlem_loss}, prog_bar=True)\n",
    "            \n",
    "            psnr_p = -psnr_loss(y, y_true, 1).cpu()\n",
    "            ssim_p = (-2 * ssim_loss(y, y_true, 5) + 1).cpu()\n",
    "            file = SAVE_TEST+\"idx_\"+str(batch_idx)+\"_psnr=\"+str(psnr_p.numpy())+\"_ssim=\"+str(ssim_p.numpy())+\".png\"\n",
    "            save_image(y, file)\n",
    "            \n",
    "            display_list = [angle, step, y, tar_img, rec_img]\n",
    "            display_func(display_list)             \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "        \n",
    "        opt_lmlem = torch.optim.Adam(self.l_mlem.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "        return opt_lmlem        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc2ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(Callback):\n",
    "    def __init__(self, every_n_epochs=5):\n",
    "        super(DisplayCallback, self).__init__()\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module, *args):\n",
    "        val_dataloader = trainer.val_dataloaders[0]\n",
    "        val_dataset = val_dataloader.dataset\n",
    "\n",
    "        d, y_true = next(iter(val_dataset))\n",
    "        \n",
    "        # Reconstruct images\n",
    "        d = d.to(pl_module.device)\n",
    "        d = d[None, :, :, :]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pl_module.eval()\n",
    "            y, _= pl_module(d)\n",
    "            pl_module.train()\n",
    "                \n",
    "            display_list = [d, y_true[None, :, :, :], y]\n",
    "            display_func(display_list)                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9f3e38",
   "metadata": {},
   "source": [
    "# **Traing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c60dfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN__Loading the dataset__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name   | Type         | Params\n",
      "----------------------------------------\n",
      "0 | l_mlem | Learned_MLEM | 10.2 K\n",
      "----------------------------------------\n",
      "10.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.2 K    Total params\n",
      "0.041     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__DONE__Loading the dataset__\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baab19e6779b4af1920416cc793b3f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 12428, 2340, 19172, 28392, 23080, 11824) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[0;32m     34\u001b[0m     model \u001b[38;5;241m=\u001b[39m LP_Learned_MLEM(N, N_RHO, N_THETA, N_ITER)\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mccst_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, ccst_data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    731\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    733\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    734\u001b[0m )\n\u001b[1;32m--> 735\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1166\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1166\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1168\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1252\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1274\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1343\u001b[0m     \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    154\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m--> 155\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:127\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[0;32m    126\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mready\n\u001b[1;32m--> 127\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     batch_idx, batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetching_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:263\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;66;03m# consume the batch we just fetched\u001b[39;00m\n\u001b[0;32m    265\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pytorch_lightning\\utilities\\fetching.py:277\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch_next_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator: Iterator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     start_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_start()\n\u001b[1;32m--> 277\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetched \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_len:\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;66;03m# when we don't prefetch but the dataloader is sized, we use the length for `done`\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1325\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1327\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1176\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1175\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 12428, 2340, 19172, 28392, 23080, 11824) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "ccst_data = CCSTDataModule()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_weights_only=True,\n",
    "    dirpath = CHECKPOINT_PATH,\n",
    "    monitor=\"val_mlem_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator='gpu', \n",
    "    devices=AVAIL_GPUS,\n",
    "    strategy=\"dp\",\n",
    "    max_epochs=10, \n",
    "    callbacks=[\n",
    "               checkpoint_callback,\n",
    "               DisplayCallback(every_n_epochs=1),\n",
    "               LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "pretrained_filename = CHECKPOINT_PATH\n",
    "ckpt_found = False\n",
    "for x in os.listdir(pretrained_filename):\n",
    "    if x.endswith(\".ckpt\"):\n",
    "        ckpt_found = True\n",
    "        pretrained_filename += x\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        break\n",
    "\n",
    "if ckpt_found:\n",
    "    model = LP_Learned_MLEM.load_from_checkpoint(pretrained_filename)\n",
    "else :\n",
    "    model = LP_Learned_MLEM(N, N_RHO, N_THETA, N_ITER)\n",
    "    trainer.fit(model, ccst_data)\n",
    "    \n",
    "trainer.test(model, ccst_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
